---
title: "Midterm Report for JSphere: Classification of The Use of JavaScript on The Web"
subtitle: "Project B for CSci 651 by John Heidemann"
author:
    - name: Steven Hé (Sīchàng)
    - name: "Mentor: Harsha V. Madhyastha"
format:
    html:
        html-math-method: katex
    pdf:
        pdf-engine: latexmk
        papersize: a4
        margin-left: 1in
        margin-right: 1in
        margin-top: 1in
        margin-bottom: 1in
        indent: 2m
        number-sections: true
bibliography: main.bib
csl: acm-sig-proceedings-long-author-list.csl
---

# Weekly Meeting Notes

<!-- Provide a URL to your weekly meeting notes. These need to be accessible to the professor. -->
Link to Weekly Meeting Notes: TODO: add this in private repo.

# Introduction

<!-- Provide an overview of the need you are addressing, what you are doing and plan to do, the novelty and new results of your work, and why this work is interesting to you personally. -->

<!-- TODO: revise this generated text. -->
JavaScript (JS) plays a central role in modern web content delivery,
accounting for approximately 25% of transferred bytes and 50% of
compute delay for the median webpage [@httparchive2024web;
@httparchive2024javascript; @goel2024sprinter].
Despite its widespread use, the reasons behind the extensive utilization of
JS remain understudied.
While industry observations suggest much of
this JS is unnecessary [@ceglowski2015website],
research efforts have primarily focused on security vulnerabilities,
privacy invasions, and performance issues [@snyder2016browser;
@iqbal2021fingerprinting; @mardani2021horcrux].

This project, JSphere, aims to classify the functionalities of JS code on
top websites on the public Web. By understanding the common uses of JS,
we can potentially guide developers in optimizing their websites,
improve general online browsing experiences, and inform future utilization of
the Internet for serving Web content.

We propose to classify each JS file in the top 1000 websites into one or
more of the following categories, which we call spheres:

1. Frontend processing
1. DOM element generation
1. UX enhancement
1. Extensional features
1. Silent code

Our approach involves analyzing JS browser API calls when interacting with
these websites,
using a modified Chromium browser called VisibleV8 [@jueckstock2019visiblev8].
<!-- TODO: Write something decent about personal interest. -->
This work is personally interesting as
it bridges the gap between theoretical understanding of web technologies and
their practical implementation, potentially leading to more efficient and
user-friendly web experiences.

# Related Work

<!-- Summarize prior work related to yours and describe how your work compares. Include complete citations to the prior work. -->

<!-- TODO: This cannot repeat the introduction. We need more content instead. -->

<!-- TODO: Compare to prior work. -->
Our work differs from these previous studies by
providing a comprehensive classification of
have functionalities across major websites.
This classification aims to bridge the gap between performance analysis and
feature usage studies, offering insights into why so much JS is used and
how effective it is in achieving its intended purposes.

# JSphere

## Goal

<!-- Describe the goal of your project. -->

The primary goal of JSphere is to classify the functionalities of JS code on
top websites on the public Web. Specifically, we aim to
categorize each JS file in the top 1000 websites into one or more of
the following spheres:
<!-- TODO: This aim seems too specific. -->

- Frontend processing (e.g., user interaction, form validation).
- DOM element generation (including styling application).
- UX enhancement (e.g., animations and effects).
- Extensional features (e.g., authentication and analytics).
- Silent code (code that does not call any JS APIs, likely never executed).

By achieving this classification, we hope to
provide insights into the common uses of JS, which can inform developers,
improve user experiences, and guide future web development practices.

## Methods

<!-- Explain the methodology or approach you are using in your project. -->
<!-- Detail the data collection process. -->

We leverage web crawling to collect API call logs, then analyze these logs to
classify JS files on the top 1000 subdomains.

### Web Crawling

We visit and interact with webpages in a fashion used in [@snyder2016browser].
For each subdomain, we visit the root webpage, 3 second-level linked webpages,
9 third-level webpages, repeating this process five times.
After each webpage's `load` event fires, we conduct "chaos testing",
randomly interacting with the webpage's elements for 30 seconds in total.
During interactions, we block all page navigations by
fulfilling such requests with an HTML webpage containing a single JS block to
go back in history, and record this navigation attempt for the second- and
third-level linked webpages.
With the browser's back-forward cache enabled,
we find this method reliably resumes the interaction, with
infrequent glitches that we detect and solve by visiting the webpage again.
We use Playwright to automate this browsing process and inject Gremlins.js for
the chaos testing.
<!-- TODO: Shall I talk about the client location? -->

While loading and interacting with webpages,
VisibleV8 records the API calls they make and provides logs for our analysis.
VisibleV8 is a modified Chromium browser [@jueckstock2019visiblev8]; for
each Chromium thread, it writes a log file that
contains each script context and browser API call made in that
context [@jueckstock2019visiblev8].
We open a new browser page every time we visit a webpage to
ensure all API calls in
each log file before our injected interaction script are made during page
loading.

We also collect the HTTP Archive (HAR) files and a list of reachable or
visited webpages for potential further analysis.
The HAR files contain detailed information about the network requests made and
the timing, which we can use for website performance comparisons.
The list of reachable or
visited webpages helps identify the links we clicked and webpages we visited.

### Log Processing

We parse the VisibleV8 logs to aggregate the API calls of each script context.
A script context is either a standalone JS file being executed, a script tag in
an HTML file, or a script created using `eval()`.
For each context, we group API calls made based on their API types, `this`
values, and attributes accessed, e.g., a Get API on `Window`'s `scrollY`
attribute, and count the presence of each API call group.
Additionally, we filter out internal, user-defined, and
injected calls using log information and API name filtering, so we can focus on
browser API usage. Our custom Rust library powers this processing.

We divide API calls into two portions: calls made during page loading and
calls made after interaction begin.
This division helps us filter out API calls not made for interaction handling.
We make this distinction based on when the browser enters a context of
our injected interaction script in the log file.

Additional to the API call logs, we also collect the script sizes in bytes from
their source code provided in the logs.
The script sizes may be a more indicative metric than script counts because
they better reflect the transferred bytes and the execution time of
the scripts, which directly affect user experiences.

### Heuristics for Classification {#sec-heuristics}

We count specific "anchor" APIs that strongly indicate specific spheres, and
develop heuristics based on them to determine if
a script context falls under a sphere.
<!-- TODO: Describe the current heuristics. -->

The current heuristics are subject to change.

## Initial Results

<!--
For Project B, you should have at least one preliminary result.
If you have other results that are incomplete but expected for Project C,
you can also describe what you plan to do.
-->

<!-- Present any preliminary results you have obtained. If you have other results that are incomplete but expected for Project C, describe what you plan to do. -->

For our initial results, we are mostly concerned with two aspects:
validating our assumptions about a small set of
browser APIs dominating all API usage (the 20/80 rule), and testing whether
we can classify scripts into spheres based on these APIs.
Our preliminary analysis of the top 100 subdomains confirmed the 20/80 rule of
browser APIs, based on which
we developed our initial classification heuristics; however,
our initial classification results demonstrate that
scripts are too large a unit to separate spheres effectively.

### The 20/80 Rule of Browser APIs

**Dominating APIs.** We find that 1.75% (318) of all observed APIs account for
80% of all API calls and 3.74% (678) cover 90% of all API calls.
This tail-heavy distribution suggests that a relatively small set of
APIs dominates JS usage on the web.

![Cumulative distributed function of observed API calls by
increasing fraction of APIs.](data/api_calls_cdf.png){width=80%}

**"Anchor" APIs.** We manually inspected the top 678 APIs that make up 90% of
all API calls and identified "anchor" APIs that
strongly indicate specific spheres.
For example, `addEventListener` for Frontend Processing and `createElement` for
DOM Element Generation.
Using these anchor APIs, we handcrafted the initial heuristics in
@sec-heuristics to classify scripts into spheres.
<!-- TODO: merge with above. -->

### Initial Classification Results

<!-- TODO: Overview of the classification results. -->
No significant correlation was found between script size and classification, or
any of the spheres.

**Single-Category Results.** We apply our heuristics to the 40,116 scripts from
the top 100 subdomains, totaling 3,192.7 MB.
@tbl-script-classification shows our heuristics successfully categorized 93% of
script bytes into at least one sphere (inverse of "No Sure Sphere"), though
it is only 67% of scripts by count.
<!-- TODO: Explain the table more. -->

| Feature                | Count | Count (%) | Size (MB) | Size (%) |
| ---------------------- | ----- | --------- | --------- | -------- |
| Total Scripts          | 40116 | -         | 3192.7    | -        |
| Frontend Processing    | 14129 | 35.22     | 2864.3    | 89.72    |
| DOM Element Generation | 8196  | 20.43     | 2248.9    | 70.44    |
| UX Enhancement         | 4496  | 11.21     | 1840.7    | 57.65    |
| Extensional Features   | 4915  | 12.25     | 1888.1    | 59.14    |
| Silent Scripts         | 10260 | 25.58     | 28.1      | 0.88     |
| Has Request            | 4205  | 10.48     | 1432.1    | 44.86    |
| Queries Element        | 13640 | 34.00     | 2731.6    | 85.56    |
| Uses Storage           | 4571  | 11.39     | 1641.0    | 51.40    |
| No Sure Sphere         | 13148 | 32.77     | 221.1     | 6.93     |
| No Category            | 10178 | 25.37     | 179.7     | 5.63     |

: Initial classification results for the top 100 subdomains: the script counts,
sizes and their corresponding percentages out of the respective total.
{#tbl-script-classification}

**Script Size Distribution.** Script sizes range from 9 bytes to 8.7 MB.
The average script size is 80 kB, with a median of 2.2 kB.

<!-- Corresponding to this in the proposal:
Then, we plan to report the distributions of the number of bytes of
JS files (bytes of JS) across the spheres. -->
<!-- TODO: Better caption. -->
![Cumulative fraction of all scripts and scripts in each sphere out of
the total number of scripts in each category, as
script size increases.](data/script_size_cdf.png){width=80%}

**Multi-Sphere Scripts.** Many large scripts belong to multiple spheres.
@tbl-2-sphere-scripts shows high overlap between and among spheres: if
we pick any two spheres, we find around half of all scripts by size belong to
those spheres.
In fact, 1296.1 MB of scripts (40.60% by size) fall into all sure spheres,
although the count is only 1473 (3.67%).
The all-sure-sphere scripts have sizes ranging from 7.1 kB to 8.7 MB,
suggesting a small fraction of large scripts tightly couples multiple spheres.
Therefore, classifying by each script file is not fine-grained enough to
meaningfully separate spheres.

| Feature Combination        | Frontend Processing              | DOM Element Generation          | UX Enhancement                  |
| -------------------------- | -------------------------------- | ------------------------------- | ------------------------------- |
| **DOM Element Generation** | 6602 (16.46%), 2197.1MB (68.82%) | -                               | -                               |
| **UX Enhancement**         | 3840 (9.57%), 1821.5MB (57.05%)  | 3125 (7.79%), 1613.2MB (50.53%) | -                               |
| **Extensional Features**   | 4229 (10.54%), 1841.1MB (57.67%) | 2703 (6.74%), 1562.9MB (48.95%) | 1844 (4.60%), 1440.5MB (45.12%) |

: Counts (with percentages in parentheses) and sizes (with percentages) of
scripts that belong to two spheres. {#tbl-2-sphere-scripts}

<!--
| Feature Combination                                                     | Scripts Count (%) | Size (MB) (%)     |
| ----------------------------------------------------------------------- | ----------------- | ----------------- |
| **Frontend Processing & DOM Element Generation & UX Enhancement**       | 2813 (7.01%)      | 1604.9MB (50.27%) |
| **Frontend Processing & DOM Element Generation & Extensional Features** | 2679 (6.68%)      | 1533.0MB (48.02%) |
| **Frontend Processing & UX Enhancement & Extensional Features**         | 1814 (4.52%)      | 1439.2MB (45.08%) |
| **DOM Element Generation & UX Enhancement & Extensional Features**      | 1482 (3.69%)      | 1296.6MB (40.61%) |

: Counts (with percentages in parentheses) and sizes (with percentages) of
scripts that belong to three spheres. {#tbl-3-sphere-scripts}
-->

# Next Steps for Research Project C

<!-- Describe the next steps for Research Project C. -->

To meaningfully classify functionalities of JS on the Web, we need to focus on
addressing two main challenges:

1. How to grind down the granularity of classification unit to
    separate different spheres?
1. How to ensure high classification accuracy?

Based on our improved sphere classifications, we will then analyze overall and
per-website sphere distributions, and their relationships with
scripts sizes and load times.

## Classification Granularity Improvement

## Heuristics Validation and Refinement

<!-- TODO: Case studies on popular APIs to see what they are used for. -->

## Aggregate and Correlation Analysis

<!-- TODO: WIP -->
We plan to expand our analysis to the full set of 1000 top websites.
Additionally, to associate the JS sphere classifications with
website performance…

<!-- TODO: Address these:
the sphere distributions across websites, and
the overall JS-sphere relationships as a Venn diagram.
Additionally, we may associate the bytes of JS per sphere with
each webpage's page load time, including time to first byte and time to
interactive. -->

These additional analyses will provide a more comprehensive understanding of
JS usage on the modern web and may reveal opportunities for optimization and
improved development practices.

## Checklist for Deliverables

<!-- Include a checklist of specific deliverables for Project C:
- Continuing to meet once a week with your mentor
- Continued weekly notes about your progress
- Expected end results (code or experiments)
- Project C report -->

# References {.unnumbered}
<!-- Include a bibliography with at least the citations from the related work section. -->
